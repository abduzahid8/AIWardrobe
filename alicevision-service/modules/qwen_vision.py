"""
ðŸ§  QWEN2.5-VL - The Golden Path Analysis Engine
=================================================

REPLACES Florence-2 with Qwen2.5-VL for:
- Native resolution processing (4K capable)
- Structured JSON output for database-ready attributes
- True reasoning about fashion, not just tagging
- Temporal understanding from video frames

This is the LATE 2025 SOTA for fashion analysis.
"""

import cv2
import numpy as np
import torch
from PIL import Image
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field, asdict
import logging
import time
import base64
import json
import io

logger = logging.getLogger(__name__)


# ============================================
# ðŸŽ¯ STRUCTURED OUTPUT SCHEMAS
# ============================================

FASHION_ANALYSIS_SCHEMA = {
    "type": "object",
    "properties": {
        "category": {
            "type": "string",
            "description": "Main category: Top, Bottom, Dress, Outerwear, Footwear, Accessory"
        },
        "specific_type": {
            "type": "string",
            "description": "Specific type like 'denim trucker jacket', 'silk blouse'"
        },
        "attributes": {
            "type": "object",
            "properties": {
                "neckline": {"type": "string"},
                "sleeve_length": {"type": "string"},
                "sleeve_style": {"type": "string"},
                "fit": {"type": "string"},
                "length": {"type": "string"},
                "closure": {"type": "string"}
            }
        },
        "material": {
            "type": "object",
            "properties": {
                "primary": {"type": "string"},
                "texture": {"type": "string"},
                "weight": {"type": "string"},
                "sheen": {"type": "string"}
            }
        },
        "pattern": {
            "type": "object",
            "properties": {
                "type": {"type": "string"},
                "direction": {"type": "string"},
                "scale": {"type": "string"}
            }
        },
        "colors": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "name": {"type": "string"},
                    "hex": {"type": "string"},
                    "percentage": {"type": "number"}
                }
            }
        },
        "style": {
            "type": "object",
            "properties": {
                "aesthetic": {"type": "string"},
                "occasion": {"type": "array", "items": {"type": "string"}},
                "season": {"type": "array", "items": {"type": "string"}}
            }
        },
        "confidence": {"type": "number"}
    },
    "required": ["category", "specific_type", "attributes", "material", "pattern", "colors"]
}


SYSTEM_PROMPT = """You are a technical fashion analyst with expertise in:
- Fabric identification (distinguishing silk from satin, linen from cotton)
- Construction details (seams, darts, pleats, gathers)
- Pattern recognition (weave patterns, prints, textures)
- Style classification (aesthetic movements, era identification)

When analyzing garments:
1. LOOK at fabric texture - how light reflects indicates material
2. EXAMINE construction - visible seams, buttons, zippers indicate quality
3. IDENTIFY pattern - stripes, checks, florals have specific names
4. CLASSIFY style - where and when would this be worn

Always output structured JSON that is database-ready.
Be specific: say "brushed cotton flannel" not just "cotton".
"""


@dataclass
class QwenFashionAnalysis:
    """Structured analysis from Qwen2.5-VL"""
    category: str
    specific_type: str
    
    # Attributes
    neckline: Optional[str] = None
    sleeve_length: Optional[str] = None
    sleeve_style: Optional[str] = None
    fit: Optional[str] = None
    length: Optional[str] = None
    closure: Optional[str] = None
    
    # Material
    material_primary: str = "unknown"
    material_texture: Optional[str] = None
    material_weight: Optional[str] = None
    material_sheen: Optional[str] = None
    
    # Pattern
    pattern_type: str = "solid"
    pattern_direction: Optional[str] = None
    pattern_scale: Optional[str] = None
    
    # Colors
    colors: List[Dict] = field(default_factory=list)
    
    # Style
    aesthetic: Optional[str] = None
    occasions: List[str] = field(default_factory=list)
    seasons: List[str] = field(default_factory=list)
    
    # Confidence
    confidence: float = 0.0
    
    # Bounding box
    bbox: Optional[List[int]] = None
    
    def to_dict(self) -> Dict:
        return asdict(self)


class QwenVisionAnalyzer:
    """
    ðŸ§  Qwen2.5-VL Fashion Analyzer
    
    The "Golden Path" analysis engine for LATE 2025.
    
    Key advantages over Florence-2:
    1. NATIVE RESOLUTION: Processes 4K without downscaling
    2. STRUCTURED JSON: Database-ready output, not text blobs
    3. TRUE REASONING: Can infer fabric from light reflection
    4. TEMPORAL: Can analyze video sequences for fabric drape
    
    Usage:
        analyzer = QwenVisionAnalyzer()
        result = analyzer.analyze(image)
        print(f"Type: {result.specific_type}")
        print(f"Material: {result.material_primary}")
    """
    
    def __init__(
        self,
        model_size: str = "7b",
        quantization: Optional[str] = None,  # "4bit", "8bit", or None
        device: str = "auto"
    ):
        """
        Initialize Qwen2.5-VL analyzer.
        
        Args:
            model_size: "7b" or "72b" (72b for maximum accuracy)
            quantization: None, "4bit", or "8bit" for reduced memory
            device: "auto", "cuda", "mps", or "cpu"
        """
        self.model_size = model_size
        self.quantization = quantization
        
        # Device setup
        self.device = self._get_device(device)
        
        # Model instances (lazy loaded)
        self._model = None
        self._processor = None
        
        logger.info(f"ðŸ§  QwenVisionAnalyzer initialized ({model_size}, device={self.device})")
    
    def _get_device(self, device: str) -> str:
        """Get best available device."""
        if device != "auto":
            return device
        
        if torch.cuda.is_available():
            return "cuda"
        elif torch.backends.mps.is_available():
            # Qwen may work on MPS with some limitations
            return "cpu"  # Safer fallback
        return "cpu"
    
    def _load_model(self):
        """Load Qwen2.5-VL model."""
        if self._model is not None:
            return
        
        try:
            from transformers import Qwen2VLForConditionalGeneration, AutoProcessor
            
            model_id = f"Qwen/Qwen2.5-VL-{self.model_size.upper()}-Instruct"
            logger.info(f"ðŸ“¥ Loading {model_id}...")
            
            # Configure quantization
            load_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if self.device == "cuda" else torch.float32
            }
            
            if self.quantization == "4bit":
                from transformers import BitsAndBytesConfig
                load_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_4bit=True,
                    bnb_4bit_compute_dtype=torch.float16
                )
            elif self.quantization == "8bit":
                from transformers import BitsAndBytesConfig
                load_kwargs["quantization_config"] = BitsAndBytesConfig(
                    load_in_8bit=True
                )
            
            self._processor = AutoProcessor.from_pretrained(
                model_id,
                trust_remote_code=True
            )
            
            self._model = Qwen2VLForConditionalGeneration.from_pretrained(
                model_id,
                **load_kwargs
            )
            
            if self.device == "cuda" and self.quantization is None:
                self._model = self._model.cuda()
            
            self._model.eval()
            logger.info(f"âœ… Qwen2.5-VL loaded ({self.model_size})")
            
        except Exception as e:
            logger.error(f"âŒ Failed to load Qwen2.5-VL: {e}")
            logger.info("ðŸ’¡ Falling back to API-based analysis")
            raise
    
    def analyze(
        self,
        image: Union[np.ndarray, Image.Image, str],
        return_bbox: bool = False
    ) -> QwenFashionAnalysis:
        """
        Analyze a fashion image with Qwen2.5-VL.
        
        Args:
            image: BGR numpy array, PIL Image, or base64 string
            return_bbox: Whether to include bounding boxes
            
        Returns:
            QwenFashionAnalysis with structured attributes
        """
        start_time = time.time()
        
        # Convert to PIL
        pil_image = self._to_pil(image)
        
        try:
            # Try local model first
            result = self._analyze_local(pil_image, return_bbox)
        except Exception as e:
            logger.warning(f"Local Qwen failed: {e}, trying API fallback...")
            result = self._analyze_api_fallback(pil_image)
        
        result.confidence = round(result.confidence, 3)
        
        processing_time = (time.time() - start_time) * 1000
        logger.info(f"ðŸ§  Qwen analysis complete: {result.specific_type} in {processing_time:.0f}ms")
        
        return result
    
    def _to_pil(self, image: Union[np.ndarray, Image.Image, str]) -> Image.Image:
        """Convert various image formats to PIL."""
        if isinstance(image, Image.Image):
            return image
        elif isinstance(image, np.ndarray):
            return Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        elif isinstance(image, str):
            # Base64
            if image.startswith("data:"):
                image = image.split(",")[1]
            img_bytes = base64.b64decode(image)
            return Image.open(io.BytesIO(img_bytes))
        else:
            raise ValueError(f"Unsupported image type: {type(image)}")
    
    def _analyze_local(
        self,
        image: Image.Image,
        return_bbox: bool
    ) -> QwenFashionAnalysis:
        """Analyze using local Qwen2.5-VL model."""
        self._load_model()
        
        # Build prompt
        prompt = f"""Analyze this garment image and extract fashion attributes.

Output your analysis as a JSON object with this structure:
{{
    "category": "Top/Bottom/Dress/Outerwear/Footwear/Accessory",
    "specific_type": "detailed type like 'denim trucker jacket'",
    "attributes": {{
        "neckline": "crew/v-neck/boat/etc",
        "sleeve_length": "short/3-4/long/sleeveless",
        "sleeve_style": "set-in/raglan/bishop/etc",
        "fit": "slim/regular/relaxed/oversized",
        "length": "cropped/regular/long",
        "closure": "button/zip/pullover/etc"
    }},
    "material": {{
        "primary": "cotton/silk/wool/polyester/etc",
        "texture": "smooth/brushed/ribbed/etc",
        "weight": "lightweight/medium/heavy",
        "sheen": "matte/slight/high"
    }},
    "pattern": {{
        "type": "solid/stripes/plaid/floral/etc",
        "direction": "horizontal/vertical/diagonal/none",
        "scale": "fine/medium/bold"
    }},
    "colors": [
        {{"name": "navy blue", "hex": "#1a2b4c", "percentage": 80}},
        {{"name": "white", "hex": "#ffffff", "percentage": 20}}
    ],
    "style": {{
        "aesthetic": "casual/formal/streetwear/bohemian/etc",
        "occasion": ["work", "weekend"],
        "season": ["spring", "fall"]
    }},
    "confidence": 0.95
}}

Be precise about materials - distinguish silk from satin, linen from cotton.
Look at how light reflects to determine fabric type.
Only output the JSON, no other text."""

        # Create messages
        messages = [
            {"role": "system", "content": SYSTEM_PROMPT},
            {
                "role": "user",
                "content": [
                    {"type": "image", "image": image},
                    {"type": "text", "text": prompt}
                ]
            }
        ]
        
        # Process
        text = self._processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )
        
        inputs = self._processor(
            text=[text],
            images=[image],
            return_tensors="pt"
        )
        
        if self.device == "cuda":
            inputs = {k: v.cuda() if hasattr(v, 'cuda') else v for k, v in inputs.items()}
        
        # Generate
        with torch.no_grad():
            outputs = self._model.generate(
                **inputs,
                max_new_tokens=1024,
                do_sample=False
            )
        
        # Decode
        response = self._processor.decode(
            outputs[0],
            skip_special_tokens=True
        )
        
        # Parse JSON from response
        return self._parse_response(response)
    
    def _analyze_api_fallback(self, image: Image.Image) -> QwenFashionAnalysis:
        """
        Fallback to OpenAI GPT-4V API if local model unavailable.
        
        Uses the same structured output format.
        """
        import os
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("No API key, returning basic analysis")
            return self._basic_analysis(image)
        
        try:
            from openai import OpenAI
            
            client = OpenAI(api_key=api_key)
            
            # Convert to base64
            buffer = io.BytesIO()
            image.save(buffer, format="JPEG", quality=90)
            img_b64 = base64.b64encode(buffer.getvalue()).decode()
            
            response = client.chat.completions.create(
                model="gpt-4o",
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": "Analyze this garment and output structured JSON with category, specific_type, attributes, material, pattern, colors, and style."},
                            {"type": "image_url", "image_url": {"url": f"data:image/jpeg;base64,{img_b64}"}}
                        ]
                    }
                ],
                max_tokens=1000,
                response_format={"type": "json_object"}
            )
            
            content = response.choices[0].message.content
            return self._parse_response(content)
            
        except Exception as e:
            logger.error(f"API fallback failed: {e}")
            return self._basic_analysis(image)
    
    def _basic_analysis(self, image: Image.Image) -> QwenFashionAnalysis:
        """Basic analysis when no model is available."""
        # Extract dominant color
        img_array = np.array(image)
        median = np.median(img_array.reshape(-1, 3), axis=0).astype(int)
        r, g, b = median
        hex_color = f"#{r:02x}{g:02x}{b:02x}"
        
        return QwenFashionAnalysis(
            category="Clothing",
            specific_type="garment",
            colors=[{"name": "detected", "hex": hex_color, "percentage": 100}],
            confidence=0.3
        )
    
    def _parse_response(self, response: str) -> QwenFashionAnalysis:
        """Parse JSON response into structured analysis."""
        try:
            # Extract JSON from response
            json_start = response.find("{")
            json_end = response.rfind("}") + 1
            
            if json_start >= 0 and json_end > json_start:
                json_str = response[json_start:json_end]
                data = json.loads(json_str)
            else:
                data = {}
            
            # Extract fields
            attrs = data.get("attributes", {})
            material = data.get("material", {})
            pattern = data.get("pattern", {})
            style = data.get("style", {})
            
            return QwenFashionAnalysis(
                category=data.get("category", "Clothing"),
                specific_type=data.get("specific_type", "garment"),
                neckline=attrs.get("neckline"),
                sleeve_length=attrs.get("sleeve_length"),
                sleeve_style=attrs.get("sleeve_style"),
                fit=attrs.get("fit"),
                length=attrs.get("length"),
                closure=attrs.get("closure"),
                material_primary=material.get("primary", "unknown"),
                material_texture=material.get("texture"),
                material_weight=material.get("weight"),
                material_sheen=material.get("sheen"),
                pattern_type=pattern.get("type", "solid"),
                pattern_direction=pattern.get("direction"),
                pattern_scale=pattern.get("scale"),
                colors=data.get("colors", []),
                aesthetic=style.get("aesthetic"),
                occasions=style.get("occasion", []),
                seasons=style.get("season", []),
                confidence=data.get("confidence", 0.8)
            )
            
        except json.JSONDecodeError as e:
            logger.warning(f"Failed to parse JSON response: {e}")
            return QwenFashionAnalysis(
                category="Clothing",
                specific_type="garment",
                confidence=0.3
            )
    
    def analyze_video(
        self,
        frames: List[np.ndarray],
        sample_rate: int = 5
    ) -> QwenFashionAnalysis:
        """
        Analyze video frames with temporal understanding.
        
        Qwen2.5-VL supports native video input, allowing inference
        of fabric stiffness from how the garment moves.
        
        Args:
            frames: List of BGR frames
            sample_rate: Sample every N frames
            
        Returns:
            Aggregated analysis from temporal observation
        """
        # Sample frames
        sampled = frames[::sample_rate][:10]  # Max 10 frames
        
        # Analyze each frame
        analyses = []
        for frame in sampled:
            try:
                result = self.analyze(frame)
                analyses.append(result)
            except Exception as e:
                logger.warning(f"Frame analysis failed: {e}")
        
        if not analyses:
            return QwenFashionAnalysis(
                category="Clothing",
                specific_type="garment",
                confidence=0.0
            )
        
        # Aggregate results (vote for most common values)
        return self._aggregate_analyses(analyses)
    
    def _aggregate_analyses(
        self,
        analyses: List[QwenFashionAnalysis]
    ) -> QwenFashionAnalysis:
        """Aggregate multiple frame analyses."""
        from collections import Counter
        
        # Vote for most common values
        categories = Counter(a.category for a in analyses)
        types = Counter(a.specific_type for a in analyses)
        materials = Counter(a.material_primary for a in analyses)
        patterns = Counter(a.pattern_type for a in analyses)
        
        # Get most common
        best = analyses[0]
        
        return QwenFashionAnalysis(
            category=categories.most_common(1)[0][0],
            specific_type=types.most_common(1)[0][0],
            neckline=best.neckline,
            sleeve_length=best.sleeve_length,
            sleeve_style=best.sleeve_style,
            fit=best.fit,
            length=best.length,
            closure=best.closure,
            material_primary=materials.most_common(1)[0][0],
            material_texture=best.material_texture,
            material_weight=best.material_weight,
            material_sheen=best.material_sheen,
            pattern_type=patterns.most_common(1)[0][0],
            pattern_direction=best.pattern_direction,
            pattern_scale=best.pattern_scale,
            colors=best.colors,
            aesthetic=best.aesthetic,
            occasions=best.occasions,
            seasons=best.seasons,
            confidence=sum(a.confidence for a in analyses) / len(analyses)
        )


# ============================================
# ðŸ”§ SINGLETON INSTANCE
# ============================================

_qwen_analyzer = None


def get_qwen_analyzer() -> QwenVisionAnalyzer:
    """Get singleton analyzer instance."""
    global _qwen_analyzer
    if _qwen_analyzer is None:
        _qwen_analyzer = QwenVisionAnalyzer()
    return _qwen_analyzer


def analyze_with_qwen(image: Union[np.ndarray, Image.Image]) -> Dict:
    """Quick utility for Qwen analysis."""
    analyzer = get_qwen_analyzer()
    result = analyzer.analyze(image)
    return result.to_dict()


def detect_material(image: Union[np.ndarray, Image.Image, str]) -> Dict[str, Any]:
    """
    ðŸŽ¯ MATERIAL DETECTION HELPER
    
    Quick function to extract just the material information from an image.
    Useful for enhancing product card prompts with accurate material descriptions.
    
    Args:
        image: BGR numpy array, PIL Image, or base64 string
        
    Returns:
        Dict with material info: {
            "primary": "cotton",
            "texture": "brushed",
            "weight": "medium",
            "sheen": "matte",
            "description": "brushed cotton fabric"
        }
    """
    try:
        analyzer = get_qwen_analyzer()
        result = analyzer.analyze(image)
        
        # Build description
        parts = []
        if result.material_texture:
            parts.append(result.material_texture)
        if result.material_primary:
            parts.append(result.material_primary)
        if result.material_weight:
            parts.append(f"({result.material_weight} weight)")
        
        description = " ".join(parts) if parts else result.material_primary or "fabric"
        
        return {
            "primary": result.material_primary or "fabric",
            "texture": result.material_texture,
            "weight": result.material_weight,
            "sheen": result.material_sheen,
            "description": description,
            "confidence": result.confidence
        }
        
    except Exception as e:
        logger.warning(f"Material detection failed: {e}, returning default")
        return {
            "primary": "fabric",
            "texture": None,
            "weight": None,
            "sheen": None,
            "description": "fabric",
            "confidence": 0.3
        }

