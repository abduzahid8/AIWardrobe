"""
ðŸš€ SAM 2 Segmentation Module - Next-Generation Clothing Segmentation
Implements Meta's Segment Anything Model 2 with Hiera encoder

Key Features:
1. Hiera (Hierarchical ViT) encoder for multi-scale features
2. Memory mechanism for consistent video segmentation
3. Promptable interface (point, box, text via Grounding DINO integration)
4. Zero-shot capability for any garment type
"""

import cv2
import numpy as np
from typing import List, Dict, Optional, Tuple, Any, Union
from dataclasses import dataclass, field
import logging
import base64
from PIL import Image
from io import BytesIO
import time

logger = logging.getLogger(__name__)


# ============================================
# ðŸŽ¯ DATA STRUCTURES
# ============================================

@dataclass
class SAM2Mask:
    """Individual segmentation mask from SAM 2"""
    mask: np.ndarray                    # Binary mask (H, W)
    confidence: float                   # Prediction confidence
    bbox: Tuple[int, int, int, int]     # Bounding box (x1, y1, x2, y2)
    area: int                           # Mask area in pixels
    stability_score: float = 0.0        # Mask stability score
    
    def to_dict(self) -> Dict:
        return {
            "bbox": list(self.bbox),
            "confidence": round(self.confidence, 4),
            "area": self.area,
            "stabilityScore": round(self.stability_score, 4)
        }


@dataclass
class SAM2Result:
    """Complete SAM 2 segmentation result"""
    masks: List[SAM2Mask]
    combined_mask: Optional[np.ndarray] = None
    processing_time_ms: float = 0.0
    prompt_type: str = "auto"
    model_variant: str = "base"
    
    def to_dict(self) -> Dict:
        return {
            "masksCount": len(self.masks),
            "masks": [m.to_dict() for m in self.masks],
            "processingTimeMs": round(self.processing_time_ms, 2),
            "promptType": self.prompt_type,
            "modelVariant": self.model_variant
        }


@dataclass
class VideoSegmentationResult:
    """Result of video segmentation with memory tracking"""
    frames: List[SAM2Result]
    object_ids: List[int]
    temporal_consistency: float
    total_processing_time_ms: float
    
    def to_dict(self) -> Dict:
        return {
            "framesCount": len(self.frames),
            "objectIds": self.object_ids,
            "temporalConsistency": round(self.temporal_consistency, 4),
            "totalProcessingTimeMs": round(self.total_processing_time_ms, 2)
        }


# ============================================
# ðŸš€ SAM 2 SEGMENTER
# ============================================

class SAM2Segmenter:
    """
    Meta's Segment Anything Model 2 for fashion segmentation.
    
    Features:
    - Hiera (Hierarchical ViT) encoder for multi-scale features
    - Memory bank for consistent video segmentation
    - Promptable interface (point, box, automatic)
    - Superior mask quality for complex garments (lace, mesh, fur)
    
    Model Variants:
    - tiny: Fastest, for real-time applications
    - base: Balanced speed and accuracy
    - large: Best quality, for studio/catalog use
    """
    
    # Replicate model endpoints
    REPLICATE_MODELS = {
        "tiny": "meta/sam-2-video:latest",
        "base": "meta/sam-2:latest",
        "large": "meta/sam-2:latest"
    }
    
    def __init__(
        self,
        model_size: str = "base",
        use_replicate: bool = True,
        device: str = "auto"
    ):
        """
        Initialize SAM 2 Segmenter.
        
        Args:
            model_size: "tiny", "base", or "large"
            use_replicate: Use Replicate API (recommended for production)
            device: Compute device for local inference
        """
        self.model_size = model_size
        self.use_replicate = use_replicate
        self.device = self._setup_device(device)
        
        # Lazy-loaded components
        self._sam_model = None
        self._sam_predictor = None
        self._memory_bank = {}  # For video tracking
        
        # Replicate API token
        self._replicate_token = None
        
        logger.info(f"SAM2Segmenter initialized (size: {model_size}, replicate: {use_replicate})")
    
    def _setup_device(self, device: str) -> str:
        """Setup compute device."""
        if device == "auto":
            try:
                import torch
                if torch.cuda.is_available():
                    return "cuda"
                elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                    return "mps"
            except ImportError:
                pass
            return "cpu"
        return device
    
    def _get_replicate_token(self) -> str:
        """Get Replicate API token."""
        if self._replicate_token is None:
            import os
            self._replicate_token = os.environ.get("REPLICATE_API_TOKEN", "")
            
            # Fallback to hardcoded token (for development)
            if not self._replicate_token:
                logger.warning("REPLICATE_API_TOKEN not found in environment variable")
        
        return self._replicate_token
    
    def _load_local_model(self):
        """Load SAM 2 model locally (requires sam2 package)."""
        if self._sam_model is not None:
            return
        
        try:
            from sam2.build_sam import build_sam2
            from sam2.sam2_image_predictor import SAM2ImagePredictor
            
            # Model checkpoint paths
            checkpoints = {
                "tiny": "sam2_hiera_tiny.pt",
                "base": "sam2_hiera_base.pt",
                "large": "sam2_hiera_large.pt"
            }
            
            checkpoint = checkpoints.get(self.model_size, "sam2_hiera_base.pt")
            
            logger.info(f"Loading SAM 2 model locally ({self.model_size})...")
            
            self._sam_model = build_sam2(
                cfg_file=f"sam2_hiera_{self.model_size}.yaml",
                ckpt_path=f"weights/{checkpoint}",
                device=self.device
            )
            
            self._sam_predictor = SAM2ImagePredictor(self._sam_model)
            
            logger.info("SAM 2 model loaded successfully")
            
        except ImportError as e:
            logger.warning(f"Local SAM 2 not available: {e}. Using Replicate API.")
            self.use_replicate = True
        except Exception as e:
            logger.warning(f"Failed to load local SAM 2: {e}. Using Replicate API.")
            self.use_replicate = True
    
    # ============================================
    # ðŸŽ¯ SEGMENTATION METHODS
    # ============================================
    
    def segment_with_prompt(
        self,
        image: np.ndarray,
        prompt_type: str = "auto",
        prompt_value: Any = None,
        multimask_output: bool = True
    ) -> SAM2Result:
        """
        Segment using SAM 2 with the given prompt.
        
        Args:
            image: BGR image
            prompt_type: "point", "box", "auto"
            prompt_value: 
                - For "point": (x, y) or [(x, y), ...]
                - For "box": (x1, y1, x2, y2)
                - For "auto": None (automatic mask generation)
            multimask_output: Return multiple candidate masks
            
        Returns:
            SAM2Result with segmentation masks
        """
        start_time = time.time()
        
        if self.use_replicate:
            result = self._segment_via_replicate(image, prompt_type, prompt_value)
        else:
            result = self._segment_locally(image, prompt_type, prompt_value, multimask_output)
        
        result.processing_time_ms = (time.time() - start_time) * 1000
        result.prompt_type = prompt_type
        result.model_variant = self.model_size
        
        return result
    
    def _segment_via_replicate(
        self,
        image: np.ndarray,
        prompt_type: str,
        prompt_value: Any
    ) -> SAM2Result:
        """Segment using Replicate API."""
        import replicate
        import requests
        
        # Convert image to base64
        if len(image.shape) == 3:
            pil_image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
        else:
            pil_image = Image.fromarray(image)
        
        buffer = BytesIO()
        pil_image.save(buffer, format="PNG")
        image_b64 = base64.b64encode(buffer.getvalue()).decode()
        image_uri = f"data:image/png;base64,{image_b64}"
        
        # Prepare input based on prompt type
        input_data = {
            "image": image_uri,
            "multimask_output": True
        }
        
        if prompt_type == "point" and prompt_value:
            if isinstance(prompt_value, tuple):
                input_data["point_coords"] = [[prompt_value[0], prompt_value[1]]]
                input_data["point_labels"] = [1]  # 1 = foreground
            elif isinstance(prompt_value, list):
                input_data["point_coords"] = [[p[0], p[1]] for p in prompt_value]
                input_data["point_labels"] = [1] * len(prompt_value)
        
        elif prompt_type == "box" and prompt_value:
            input_data["box"] = list(prompt_value)
        
        try:
            # Run prediction
            output = replicate.run(
                "meta/sam-2:latest",
                input=input_data
            )
            
            # Parse output
            masks = []
            
            if isinstance(output, dict) and "masks" in output:
                for i, mask_url in enumerate(output.get("masks", [])):
                    # Download mask
                    response = requests.get(mask_url)
                    mask_image = Image.open(BytesIO(response.content))
                    mask_array = np.array(mask_image) > 127
                    
                    # Calculate properties
                    bbox = self._mask_to_bbox(mask_array)
                    area = int(np.sum(mask_array))
                    
                    masks.append(SAM2Mask(
                        mask=mask_array.astype(np.uint8) * 255,
                        confidence=output.get("scores", [0.9])[i] if i < len(output.get("scores", [])) else 0.9,
                        bbox=bbox,
                        area=area,
                        stability_score=0.9
                    ))
            
            # Combine masks
            combined = None
            if masks:
                combined = np.zeros_like(masks[0].mask)
                for m in masks:
                    combined = np.maximum(combined, m.mask)
            
            return SAM2Result(masks=masks, combined_mask=combined)
            
        except Exception as e:
            logger.error(f"Replicate SAM 2 failed: {e}")
            return SAM2Result(masks=[])
    
    def _segment_locally(
        self,
        image: np.ndarray,
        prompt_type: str,
        prompt_value: Any,
        multimask_output: bool
    ) -> SAM2Result:
        """Segment using local SAM 2 model."""
        self._load_local_model()
        
        if self._sam_predictor is None:
            logger.error("SAM 2 model not available")
            return SAM2Result(masks=[])
        
        try:
            # Set image
            if len(image.shape) == 3 and image.shape[2] == 3:
                rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
            else:
                rgb_image = image
            
            self._sam_predictor.set_image(rgb_image)
            
            # Prepare prompts
            point_coords = None
            point_labels = None
            box = None
            
            if prompt_type == "point" and prompt_value:
                import torch
                if isinstance(prompt_value, tuple):
                    point_coords = torch.tensor([[prompt_value]], device=self.device)
                    point_labels = torch.tensor([[1]], device=self.device)
                else:
                    point_coords = torch.tensor([prompt_value], device=self.device)
                    point_labels = torch.tensor([[1] * len(prompt_value)], device=self.device)
            
            elif prompt_type == "box" and prompt_value:
                import torch
                box = torch.tensor([prompt_value], device=self.device)
            
            # Predict
            masks, scores, _ = self._sam_predictor.predict(
                point_coords=point_coords,
                point_labels=point_labels,
                box=box,
                multimask_output=multimask_output
            )
            
            # Process results
            sam_masks = []
            for i, (mask, score) in enumerate(zip(masks, scores)):
                mask_uint8 = (mask * 255).astype(np.uint8)
                bbox = self._mask_to_bbox(mask)
                area = int(np.sum(mask))
                
                sam_masks.append(SAM2Mask(
                    mask=mask_uint8,
                    confidence=float(score),
                    bbox=bbox,
                    area=area,
                    stability_score=float(score)
                ))
            
            # Combine masks
            combined = None
            if sam_masks:
                combined = np.zeros_like(sam_masks[0].mask)
                for m in sam_masks:
                    combined = np.maximum(combined, m.mask)
            
            return SAM2Result(masks=sam_masks, combined_mask=combined)
            
        except Exception as e:
            logger.error(f"Local SAM 2 segmentation failed: {e}")
            return SAM2Result(masks=[])
    
    def _mask_to_bbox(self, mask: np.ndarray) -> Tuple[int, int, int, int]:
        """Convert binary mask to bounding box."""
        if mask.sum() == 0:
            return (0, 0, 0, 0)
        
        rows = np.any(mask, axis=1)
        cols = np.any(mask, axis=0)
        
        y1, y2 = np.where(rows)[0][[0, -1]]
        x1, x2 = np.where(cols)[0][[0, -1]]
        
        return (int(x1), int(y1), int(x2), int(y2))
    
    # ============================================
    # ðŸŽ¬ VIDEO SEGMENTATION WITH MEMORY
    # ============================================
    
    def segment_video_with_memory(
        self,
        frames: List[np.ndarray],
        initial_prompt: Tuple[str, Any],
        tracking_ids: List[int] = None
    ) -> VideoSegmentationResult:
        """
        Segment and track objects across video frames using SAM 2 memory.
        
        Args:
            frames: List of video frames (BGR)
            initial_prompt: (prompt_type, prompt_value) for first frame
            tracking_ids: Optional object IDs to track
            
        Returns:
            VideoSegmentationResult with consistent masks across frames
        """
        start_time = time.time()
        
        if not frames:
            return VideoSegmentationResult(
                frames=[],
                object_ids=[],
                temporal_consistency=0.0,
                total_processing_time_ms=0.0
            )
        
        results = []
        object_ids = tracking_ids or [1]  # Default single object
        
        # Segment first frame with prompt
        prompt_type, prompt_value = initial_prompt
        first_result = self.segment_with_prompt(
            frames[0], prompt_type, prompt_value
        )
        results.append(first_result)
        
        if not first_result.masks:
            logger.warning("No masks in first frame, cannot track")
            return VideoSegmentationResult(
                frames=results,
                object_ids=object_ids,
                temporal_consistency=0.0,
                total_processing_time_ms=(time.time() - start_time) * 1000
            )
        
        # Initialize memory with first frame mask
        self._memory_bank = {
            "reference_mask": first_result.masks[0].mask,
            "reference_features": None  # Would store encoder features
        }
        
        # Track through remaining frames
        for i, frame in enumerate(frames[1:], 1):
            # Use previous mask centroid as point prompt for continuity
            prev_mask = results[-1].masks[0].mask if results[-1].masks else None
            
            if prev_mask is not None:
                # Find centroid of previous mask
                centroid = self._get_mask_centroid(prev_mask)
                frame_result = self.segment_with_prompt(
                    frame, "point", centroid
                )
            else:
                # Fallback to auto
                frame_result = self.segment_with_prompt(frame, "auto", None)
            
            results.append(frame_result)
        
        # Calculate temporal consistency
        consistency = self._calculate_temporal_consistency(results)
        
        total_time = (time.time() - start_time) * 1000
        
        return VideoSegmentationResult(
            frames=results,
            object_ids=object_ids,
            temporal_consistency=consistency,
            total_processing_time_ms=total_time
        )
    
    def _get_mask_centroid(self, mask: np.ndarray) -> Tuple[int, int]:
        """Get centroid of a binary mask."""
        if mask.sum() == 0:
            return (mask.shape[1] // 2, mask.shape[0] // 2)
        
        moments = cv2.moments(mask)
        if moments["m00"] == 0:
            return (mask.shape[1] // 2, mask.shape[0] // 2)
        
        cx = int(moments["m10"] / moments["m00"])
        cy = int(moments["m01"] / moments["m00"])
        
        return (cx, cy)
    
    def _calculate_temporal_consistency(self, results: List[SAM2Result]) -> float:
        """Calculate how consistent masks are across frames."""
        if len(results) < 2:
            return 1.0
        
        ious = []
        for i in range(1, len(results)):
            prev_mask = results[i-1].combined_mask
            curr_mask = results[i].combined_mask
            
            if prev_mask is None or curr_mask is None:
                continue
            
            # Calculate IoU
            intersection = np.logical_and(prev_mask > 127, curr_mask > 127).sum()
            union = np.logical_or(prev_mask > 127, curr_mask > 127).sum()
            
            if union > 0:
                ious.append(intersection / union)
        
        return np.mean(ious) if ious else 0.0
    
    # ============================================
    # ðŸŽ¯ AUTOMATIC CLOTHING DETECTION
    # ============================================
    
    def segment_all_clothing(
        self,
        image: np.ndarray,
        min_area_ratio: float = 0.01
    ) -> SAM2Result:
        """
        Automatically segment all clothing items in an image.
        
        Uses automatic mask generation followed by filtering
        for clothing-like regions.
        """
        start_time = time.time()
        
        # Generate all masks automatically
        result = self.segment_with_prompt(image, "auto", None)
        
        if not result.masks:
            return result
        
        # Filter masks by area and position
        image_area = image.shape[0] * image.shape[1]
        min_area = int(image_area * min_area_ratio)
        max_area = int(image_area * 0.9)  # Exclude full-image masks
        
        filtered_masks = [
            m for m in result.masks
            if min_area < m.area < max_area
        ]
        
        # Sort by confidence
        filtered_masks.sort(key=lambda m: m.confidence, reverse=True)
        
        result.masks = filtered_masks
        result.processing_time_ms = (time.time() - start_time) * 1000
        
        return result
    
    def segment_with_grounding_dino(
        self,
        image: np.ndarray,
        text_prompt: str = "clothing"
    ) -> SAM2Result:
        """
        Zero-shot segmentation using Grounding DINO for detection
        followed by SAM 2 for mask generation.
        
        This enables segmentation of any describable garment.
        """
        try:
            from modules.grounded_sam import get_grounded_sam
            
            # Use existing Grounded SAM implementation
            grounded_sam = get_grounded_sam()
            result = grounded_sam.segment_clothing(
                image,
                prompts=[text_prompt]
            )
            
            # Convert to SAM2Result format
            sam_masks = []
            for det in result.detections:
                if det.mask is not None:
                    mask_uint8 = det.mask.astype(np.uint8) * 255
                    sam_masks.append(SAM2Mask(
                        mask=mask_uint8,
                        confidence=det.confidence,
                        bbox=det.bbox,
                        area=int(np.sum(det.mask)),
                        stability_score=det.confidence
                    ))
            
            combined = None
            if sam_masks:
                combined = result.combined_mask
            
            return SAM2Result(
                masks=sam_masks,
                combined_mask=combined,
                processing_time_ms=result.processing_time * 1000,
                prompt_type="text",
                model_variant=self.model_size
            )
            
        except Exception as e:
            logger.warning(f"Grounding DINO not available: {e}, using auto segmentation")
            return self.segment_all_clothing(image)


# ============================================
# ðŸ”§ UTILITY FUNCTIONS
# ============================================

def segment_clothing_sam2(
    image_b64: str,
    prompt_type: str = "auto",
    prompt_value: Any = None,
    model_size: str = "base"
) -> Dict:
    """
    Utility function to segment clothing from base64 image using SAM 2.
    
    Returns:
        Dictionary with segmentation results
    """
    # Decode image
    if ',' in image_b64:
        image_b64 = image_b64.split(',')[1]
    
    img_bytes = base64.b64decode(image_b64)
    img_array = np.frombuffer(img_bytes, dtype=np.uint8)
    image = cv2.imdecode(img_array, cv2.IMREAD_COLOR)
    
    if image is None:
        return {"error": "Could not decode image"}
    
    # Run segmentation
    segmenter = SAM2Segmenter(model_size=model_size)
    result = segmenter.segment_with_prompt(image, prompt_type, prompt_value)
    
    # Encode mask to base64 if available
    response = result.to_dict()
    
    if result.combined_mask is not None:
        _, mask_encoded = cv2.imencode('.png', result.combined_mask)
        mask_b64 = base64.b64encode(mask_encoded).decode()
        response["combinedMaskBase64"] = f"data:image/png;base64,{mask_b64}"
    
    return response


# Singleton instance
_sam2_instance = None

def get_sam2_segmenter(model_size: str = "base") -> SAM2Segmenter:
    """Get singleton instance of SAM2Segmenter."""
    global _sam2_instance
    if _sam2_instance is None or _sam2_instance.model_size != model_size:
        _sam2_instance = SAM2Segmenter(model_size=model_size)
    return _sam2_instance
