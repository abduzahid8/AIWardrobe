"""
ðŸ§  Qwen2.5-VL Cloud Reasoning Engine
Advanced Vision-Language Model for complex fashion understanding

Capabilities:
- Dynamic resolution (native aspect ratios)
- Video understanding with M-RoPE timestamps
- Structured JSON output for databases
- Visual grounding with precise coordinates
- Complex reasoning and style analysis
"""

import os
import json
import base64
import logging
from typing import Dict, List, Optional, Any, Union
from dataclasses import dataclass, field
import time

logger = logging.getLogger(__name__)


# ============================================
# ðŸ§  DATA STRUCTURES
# ============================================

@dataclass
class QwenReasoningResult:
    """Result from Qwen2.5-VL reasoning"""
    success: bool
    query: str
    
    # Structured output
    answer: str = ""
    structured_data: Dict = field(default_factory=dict)
    
    # Grounding results
    bounding_boxes: List[Dict] = field(default_factory=list)
    
    # Fashion analysis
    outfit_analysis: Dict = field(default_factory=dict)
    style_recommendations: List[str] = field(default_factory=list)
    
    # Processing info
    processing_time_ms: float = 0
    model_used: str = "qwen2.5-vl-72b"
    tokens_used: int = 0
    
    def to_dict(self) -> Dict:
        return {
            "success": self.success,
            "query": self.query,
            "answer": self.answer,
            "structuredData": self.structured_data,
            "boundingBoxes": self.bounding_boxes,
            "outfitAnalysis": self.outfit_analysis,
            "styleRecommendations": self.style_recommendations,
            "processingTimeMs": self.processing_time_ms,
            "modelUsed": self.model_used,
            "tokensUsed": self.tokens_used
        }


# ============================================
# ðŸš€ QWEN2.5-VL REASONING ENGINE
# ============================================

class QwenVLReasoning:
    """
    ðŸ§  CLOUD REASONING ENGINE
    
    Qwen2.5-VL provides advanced reasoning capabilities:
    - Understanding complex fashion relationships
    - Generating structured attribute JSON
    - Video analysis with temporal grounding
    - Style matching and recommendations
    
    Can be deployed via:
    - vLLM server (self-hosted)
    - Together.ai API
    - Replicate API
    - OpenRouter
    """
    
    # Supported providers
    PROVIDER_VLLM = "vllm"
    PROVIDER_TOGETHER = "together"
    PROVIDER_REPLICATE = "replicate"
    PROVIDER_OPENROUTER = "openrouter"
    PROVIDER_LOCAL = "local"
    
    def __init__(
        self,
        provider: str = "replicate",
        model_id: str = "qwen2.5-vl-72b",
        api_key: str = None,
        api_base: str = None
    ):
        """
        Initialize Qwen2.5-VL reasoning engine.
        
        Args:
            provider: API provider (vllm, together, replicate, openrouter, local)
            model_id: Model identifier
            api_key: API key (or from environment)
            api_base: Custom API base URL (for vLLM)
        """
        self.provider = provider
        self.model_id = model_id
        self.api_key = api_key or self._get_api_key()
        self.api_base = api_base
        
        # Fashion-specific system prompt
        self.system_prompt = self._build_system_prompt()
        
        logger.info(f"QwenVLReasoning initialized (provider={provider})")
    
    def _get_api_key(self) -> Optional[str]:
        """Get API key from environment."""
        if self.provider == "replicate":
            return os.environ.get("REPLICATE_API_TOKEN")
        elif self.provider == "together":
            return os.environ.get("TOGETHER_API_KEY")
        elif self.provider == "openrouter":
            return os.environ.get("OPENROUTER_API_KEY")
        return None
    
    def _build_system_prompt(self) -> str:
        """Build fashion-specific system prompt."""
        return """You are an expert fashion AI analyst. You analyze clothing images and videos with professional-level accuracy.

When analyzing fashion items, you must:
1. Identify exact garment types (e.g., "cropped cable-knit cardigan" not just "sweater")
2. Detect precise colors including shades (e.g., "dusty rose" not just "pink")
3. Recognize patterns (solid, striped, plaid, floral, geometric, abstract, etc.)
4. Identify materials when visible (cotton, denim, leather, silk, wool, etc.)
5. Assess style category (casual, formal, streetwear, bohemian, minimalist, etc.)
6. Note construction details (buttons, zippers, seams, pockets, collars, etc.)

Always respond with structured JSON when asked for attributes.
Be specific and professional in your analysis."""
    
    # ============================================
    # ðŸŽ¯ CORE INFERENCE METHODS
    # ============================================
    
    def query(
        self,
        image: Union[str, List[str]],
        prompt: str,
        json_output: bool = False,
        max_tokens: int = 1024
    ) -> QwenReasoningResult:
        """
        Send query to Qwen2.5-VL.
        
        Args:
            image: Base64 image or list of images (for video frames)
            prompt: User prompt
            json_output: Whether to request JSON output
            max_tokens: Maximum response tokens
            
        Returns:
            QwenReasoningResult
        """
        start_time = time.time()
        
        try:
            if self.provider == "replicate":
                response = self._query_replicate(image, prompt, max_tokens)
            elif self.provider == "together":
                response = self._query_together(image, prompt, json_output, max_tokens)
            elif self.provider == "openrouter":
                response = self._query_openrouter(image, prompt, json_output, max_tokens)
            elif self.provider == "vllm":
                response = self._query_vllm(image, prompt, json_output, max_tokens)
            else:
                response = self._query_local(image, prompt, max_tokens)
            
            processing_time = (time.time() - start_time) * 1000
            
            # Parse response
            answer = response.get("text", "")
            structured_data = {}
            
            if json_output:
                structured_data = self._extract_json(answer)
            
            return QwenReasoningResult(
                success=True,
                query=prompt,
                answer=answer,
                structured_data=structured_data,
                processing_time_ms=processing_time,
                tokens_used=response.get("tokens", 0)
            )
            
        except Exception as e:
            logger.error(f"Qwen query failed: {e}")
            return QwenReasoningResult(
                success=False,
                query=prompt,
                answer=str(e),
                processing_time_ms=(time.time() - start_time) * 1000
            )
    
    def _query_replicate(
        self,
        image: Union[str, List[str]],
        prompt: str,
        max_tokens: int
    ) -> Dict:
        """Query via Replicate API."""
        import replicate
        
        # Prepare image
        if isinstance(image, list):
            # Multiple frames - use first for now
            img_data = image[0]
        else:
            img_data = image
        
        # Ensure proper format
        if not img_data.startswith("data:"):
            img_data = f"data:image/jpeg;base64,{img_data}"
        
        # Use Qwen model on Replicate
        output = replicate.run(
            "lucataco/qwen2-vl-72b:4deea4be38fe6e7f8b8bf03966c55fb0a6b4d5e65b0e87b0da81379ffa29f186",
            input={
                "image": img_data,
                "prompt": f"{self.system_prompt}\n\nUser: {prompt}",
                "max_tokens": max_tokens
            }
        )
        
        # Concatenate output
        if isinstance(output, list):
            text = "".join(output)
        else:
            text = str(output)
        
        return {"text": text, "tokens": max_tokens}
    
    def _query_together(
        self,
        image: Union[str, List[str]],
        prompt: str,
        json_output: bool,
        max_tokens: int
    ) -> Dict:
        """Query via Together.ai API."""
        import requests
        
        # Prepare image content
        if isinstance(image, list):
            img_data = image[0]
        else:
            img_data = image
        
        if not img_data.startswith("data:"):
            img_data = f"data:image/jpeg;base64,{img_data}"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "Qwen/Qwen2.5-VL-72B-Instruct",
            "messages": [
                {
                    "role": "system",
                    "content": self.system_prompt
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": img_data}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ],
            "max_tokens": max_tokens
        }
        
        if json_output:
            payload["response_format"] = {"type": "json_object"}
        
        response = requests.post(
            "https://api.together.xyz/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        result = response.json()
        text = result["choices"][0]["message"]["content"]
        tokens = result.get("usage", {}).get("total_tokens", 0)
        
        return {"text": text, "tokens": tokens}
    
    def _query_openrouter(
        self,
        image: Union[str, List[str]],
        prompt: str,
        json_output: bool,
        max_tokens: int
    ) -> Dict:
        """Query via OpenRouter API."""
        import requests
        
        if isinstance(image, list):
            img_data = image[0]
        else:
            img_data = image
        
        if not img_data.startswith("data:"):
            img_data = f"data:image/jpeg;base64,{img_data}"
        
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": "qwen/qwen2.5-vl-72b-instruct",
            "messages": [
                {
                    "role": "system",
                    "content": self.system_prompt
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": img_data}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ],
            "max_tokens": max_tokens
        }
        
        response = requests.post(
            "https://openrouter.ai/api/v1/chat/completions",
            headers=headers,
            json=payload
        )
        
        result = response.json()
        text = result["choices"][0]["message"]["content"]
        tokens = result.get("usage", {}).get("total_tokens", 0)
        
        return {"text": text, "tokens": tokens}
    
    def _query_vllm(
        self,
        image: Union[str, List[str]],
        prompt: str,
        json_output: bool,
        max_tokens: int
    ) -> Dict:
        """Query self-hosted vLLM server."""
        import requests
        
        if isinstance(image, list):
            img_data = image[0]
        else:
            img_data = image
        
        if not img_data.startswith("data:"):
            img_data = f"data:image/jpeg;base64,{img_data}"
        
        payload = {
            "model": self.model_id,
            "messages": [
                {
                    "role": "system",
                    "content": self.system_prompt
                },
                {
                    "role": "user",
                    "content": [
                        {"type": "image_url", "image_url": {"url": img_data}},
                        {"type": "text", "text": prompt}
                    ]
                }
            ],
            "max_tokens": max_tokens
        }
        
        response = requests.post(
            f"{self.api_base}/v1/chat/completions",
            json=payload
        )
        
        result = response.json()
        text = result["choices"][0]["message"]["content"]
        
        return {"text": text, "tokens": 0}
    
    def _query_local(
        self,
        image: Union[str, List[str]],
        prompt: str,
        max_tokens: int
    ) -> Dict:
        """Query locally loaded model."""
        # Fallback to simple analysis
        logger.warning("Local Qwen model not implemented, using fallback")
        return {
            "text": f"[Analysis of image based on: {prompt}]",
            "tokens": 0
        }
    
    def _extract_json(self, text: str) -> Dict:
        """Extract JSON from response."""
        try:
            # Try direct parse
            return json.loads(text)
        except:
            pass
        
        # Try to find JSON block
        import re
        json_match = re.search(r'\{[\s\S]*\}', text)
        if json_match:
            try:
                return json.loads(json_match.group())
            except:
                pass
        
        return {}
    
    # ============================================
    # ðŸŽ¨ FASHION-SPECIFIC METHODS
    # ============================================
    
    def analyze_outfit(self, image: str) -> QwenReasoningResult:
        """
        Complete outfit analysis with structured output.
        
        Returns JSON with garment details, colors, style, etc.
        """
        prompt = """Analyze this outfit image and return a JSON object with:
{
    "garments": [
        {
            "type": "exact garment type",
            "category": "tops/bottoms/outerwear/footwear/accessories",
            "color": "primary color",
            "colors": ["all colors"],
            "pattern": "pattern type or solid",
            "material": "apparent material",
            "fit": "fitted/regular/loose/oversized",
            "details": ["notable features"]
        }
    ],
    "overallStyle": "style category",
    "occasion": "suitable occasions",
    "season": "suitable seasons",
    "colorHarmony": "color relationship analysis",
    "styleScore": 1-10
}

Be precise and professional. Return ONLY valid JSON."""
        
        result = self.query(image, prompt, json_output=True)
        
        if result.structured_data:
            result.outfit_analysis = result.structured_data
        
        return result
    
    def extract_attributes(self, image: str, garment_type: str = None) -> QwenReasoningResult:
        """
        Extract detailed attributes for database storage.
        """
        hint = f" focusing on the {garment_type}" if garment_type else ""
        
        prompt = f"""Analyze this clothing image{hint} and extract attributes as JSON:
{{
    "type": "specific garment type",
    "subType": "detailed classification",
    "primaryColor": "main color name",
    "secondaryColors": ["other colors"],
    "colorHex": "#RRGGBB approximate",
    "pattern": {{
        "type": "solid/striped/plaid/floral/etc",
        "description": "pattern details"
    }},
    "material": {{
        "type": "fabric type",
        "texture": "smooth/textured/fuzzy/etc"
    }},
    "neckline": "if applicable",
    "sleeveLength": "if applicable",
    "length": "garment length",
    "fit": "fitted/regular/loose/oversized",
    "closure": "button/zip/none/etc",
    "brand": "if visible",
    "condition": "new/good/worn/etc"
}}

Return ONLY valid JSON."""
        
        return self.query(image, prompt, json_output=True)
    
    def ground_item(self, image: str, description: str) -> QwenReasoningResult:
        """
        Find specific item by description and return bounding box.
        
        Example: ground_item(image, "the vintage leather jacket")
        """
        prompt = f"""Find "{description}" in this image.
        
Return JSON with bounding box coordinates:
{{
    "found": true/false,
    "bbox": [x1, y1, x2, y2],  // pixel coordinates
    "confidence": 0.0-1.0,
    "description": "what you found"
}}

Return ONLY valid JSON."""
        
        result = self.query(image, prompt, json_output=True)
        
        if result.structured_data.get("found"):
            result.bounding_boxes = [{
                "label": description,
                "bbox": result.structured_data.get("bbox", []),
                "confidence": result.structured_data.get("confidence", 0)
            }]
        
        return result
    
    def suggest_styles(self, image: str, context: str = None) -> QwenReasoningResult:
        """
        Get styling suggestions based on wardrobe item.
        """
        context_hint = f" for {context}" if context else ""
        
        prompt = f"""Based on this clothing item, suggest styling options{context_hint}.

Return JSON:
{{
    "item": "what this item is",
    "versatility": 1-10,
    "suggestedPairings": [
        {{"type": "garment type", "style": "style description", "color": "color suggestion"}}
    ],
    "occasions": ["suitable occasions"],
    "avoidWith": ["items to avoid pairing with"],
    "stylingTips": ["professional styling advice"]
}}

Return ONLY valid JSON."""
        
        result = self.query(image, prompt, json_output=True)
        
        if result.structured_data.get("suggestedPairings"):
            result.style_recommendations = [
                f"{p['type']}: {p['style']} ({p['color']})"
                for p in result.structured_data["suggestedPairings"]
            ]
        
        return result


# ============================================
# ðŸ”§ UTILITY FUNCTIONS
# ============================================

def analyze_with_qwen(image_b64: str, prompt: str = None) -> Dict:
    """
    Utility function for Qwen2.5-VL analysis.
    """
    reasoning = get_qwen_reasoning()
    
    if prompt:
        result = reasoning.query(image_b64, prompt)
    else:
        result = reasoning.analyze_outfit(image_b64)
    
    return result.to_dict()


def extract_attributes_qwen(image_b64: str, garment_type: str = None) -> Dict:
    """Extract structured attributes using Qwen."""
    reasoning = get_qwen_reasoning()
    result = reasoning.extract_attributes(image_b64, garment_type)
    return result.to_dict()


# Singleton instance
_qwen_instance = None

def get_qwen_reasoning(provider: str = "replicate") -> QwenVLReasoning:
    """Get singleton Qwen reasoning instance."""
    global _qwen_instance
    if _qwen_instance is None:
        _qwen_instance = QwenVLReasoning(provider=provider)
    return _qwen_instance
